{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 Discussion: Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week's discussion centers on training a decision tree model. During training, at each node, the trees choose a variable and a threshold for splitting the data into two subsets. The aim is to make splits that minimize the label entropy in the resulting child nodes. By iteratively applying this splitting process, we aim to achieve uniformity in the labels of the data points in the child nodes. For each child node, the most prevalent label is selected as the prediction. This process includes:\n",
    "\n",
    "* Revisiting the core principles of learning decision trees.\n",
    "* Importing a dataset from `scikit-learn`.\n",
    "* Generating predictions using the trained tree.\n",
    "* Measuring performance by accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This discussion on fitting decision trees is based on the following references:\n",
    "<br>\n",
    "https://youtu.be/jVh5NA9ERDA?si=-QQP_ctg8TY3IkMR\n",
    "\n",
    "https://youtu.be/Bqi7EFFvNOg?si=WMZMJoggVBBzRMWd\n",
    "\n",
    "https://github.com/patrickloeber/MLfromscratch/blob/master/mlfromscratch/decision_tree.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Decision Trees: A Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We aim to predict whether a person will go for a walk based on two factors: the duration of the walk and whether it's raining. We have the following dataset available:\n",
    "\n",
    "| Rain | Time | Walk |\n",
    "| --- | ----------- | --- |\n",
    "| 1 | 30 | No |\n",
    "| 1 | 15 | No |\n",
    "| 1 | 5 | No |\n",
    "| 0 | 10 | No |\n",
    "| 0 | 5 | No |\n",
    "| 0 | 15 | Yes |\n",
    "| 0 | 20 | Yes |\n",
    "| 0 | 25 | Yes |\n",
    "| 0 | 30 | Yes |\n",
    "| 0 | 35 | Yes |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some basic observations about the data:\n",
    "\n",
    "* There are 10 data points.\n",
    "* The dataset consists of 2 features.\n",
    "* The output is binary, indicating whether the person goes for a walk or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our objective is to fit a tree to this dataset. We begin with all data points at the root node and aim to identify the optimal feature/split point combination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When deciding between feature and split value combinations for splitting the data, we typically use a criterion that measures the effectiveness of the split. A commonly used criterion is Information Gain:\n",
    "\n",
    "Information Gain: It measures the reduction in entropy or uncertainty achieved by splitting the data on a particular feature. We want to maximize the information gain.\n",
    "\n",
    "For each feature, we iterate over possible split values and calculate the information gain for each split. We then choose the feature and split value combination that maximizes information gain. This process helps us find the most effective way to partition the data into subsets at each node of the decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know the formula for information gain is:\n",
    "<br>\n",
    "<br>\n",
    "$$\n",
    "H(\\text{parent node}) - \\frac{\\text{\\# points in left child node} * H(\\text{left child node}) + \\text{\\# points in right child node} * H(\\text{right child node})}{\\text{\\# points in parent node}},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $H(.)$ denotes the entropy of the node, computed as:\n",
    "<br>\n",
    "<br>\n",
    "$$\n",
    "H(\\text{node}) = -\\sum{p(l) log_2 (p(l))},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $l$ denotes the labels of the data points inside the node and $p(l)$ denotes the empirical probability (relative frequency) of that label within the node.\n",
    "For instance, the entropy at the root node in our illustration is determined by the data points it contains (all of them) as:\n",
    "<br>\n",
    "<br>\n",
    "$$\n",
    "H(\\text{root}) = - (\\frac{5}{10} log_2 (\\frac{5}{10}) + \\frac{5}{10} log_2 (\\frac{5}{10})) = 1,\n",
    "$$\n",
    "***note: the base of the logarithm with be the amount of classifications***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, each split leads to potentially different entropies in the left and right child nodes, resulting in varying levels of information gain. Our aim is to identify the split that yields the highest information gain in the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The algorithm outline is as follows:\n",
    "\n",
    "* Start at the top node. At each node, select the best split based on the highest information gain.\n",
    "* Iterate over all features and thresholds.\n",
    "* Save the best split feature and split value at each node.\n",
    "* Build the tree recursively.\n",
    "* Implement stopping criteria, such as maximum depth, minimum number of samples in a node, or achievement of minimum entropy (0).\n",
    "* When a leaf node is reached, store the most common class label as the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For prediction according to this scheme:\n",
    "\n",
    "* Traverse the tree recursively.\n",
    "* At each node, examine the split feature of the test data point and proceed left or right based on whether $x[\\text{feature\\_idx}]\\geq threshold$\n",
    "* Once a leaf node is reached, return the value associated with that leaf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding it up!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To code it up, let's begin by crafting a utility function that calculates the entropy within an array of labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def entropy(y):\n",
    "    hist = np.bincount(y)\n",
    "    ps = hist / len(y)\n",
    "    return -np.sum([p * np.log2(p) for p in ps if p > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `entropy` outlined above operates on the labels $y$ and performs the following steps:\n",
    "\n",
    "* It generates a histogram of labels by tallying identical values. For instance, an array like $[1,2,2,2,2]$ would yield $[1,4]$.\n",
    "* The histogram of labels is then divided by the length of the label array to obtain relative frequencies.\n",
    "* It calculates the entropy using the formula described earlier, ensuring that the logarithm of 0 is not taken. Specifically, $p log_2 (p)$ is defined as $0$ when $p=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The foundational component of our tree structure is the class `node`, as defined below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(\n",
    "        self, feature=None, threshold=None, left=None, right=None, value=None\n",
    "    ):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "\n",
    "    def is_leaf_node(self):\n",
    "        return self.value is not None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `node` class `__init__()` function is designed to five arguments:\n",
    "\n",
    "* `feature`: Represents the feature on which the node splits. Applicable to non-leaf nodes.\n",
    "\n",
    "* `threshold`: Denotes the threshold for the node's split. Applicable to non-leaf nodes.\n",
    "\n",
    "* `left`: Refers to the left child of the node. Applicable to non-leaf nodes.\n",
    "\n",
    "* `right`: Denotes the right child of the node. Applicable to non-leaf nodes.\n",
    "\n",
    "*  `value`: Represents the node's value used for prediction. Applicable to leaf nodes.\n",
    "\n",
    "The function `is_leaf_node()` returns `True` if the class attribute `value` is set, indicating that the node is a leaf node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will construct the `DecisionTree` class gradually, one function at a time. To enhance understanding, we'll initially define the functions outside the class and later assemble them within the class structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `DecisionTree` class `__init__` function is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, min_samples_split=2, max_depth=100, n_feats=None):\n",
    "    self.min_samples_split = min_samples_split\n",
    "    self.max_depth = max_depth\n",
    "    self.n_feats = n_feats\n",
    "    self.root = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `__init__()` function of the `DecisionTree` class, there are three arguments:\n",
    "\n",
    "* `min_samples_split`: Represents the minimum number of samples in a node required for further splitting.\n",
    "\n",
    "* `max_depth`: Denotes the maximum depth allowed for the tree.\n",
    "\n",
    "* `n_feats`: Indicates the number of features present in the dataset.\n",
    "\n",
    "The `root` attribute of the class is initialized to $None$ and will be assigned later during the tree fitting process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `fit` is used for fitting the decision tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, X, y):\n",
    "    self.n_feats = X.shape[1] if not self.n_feats else min(self.n_feats, X.shape[1])\n",
    "    self.root = self._grow_tree(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `fit()` takes in two arguments: input features and the output labels.\n",
    "\n",
    "* Sets the `n_feats` attribute as the number of features.\n",
    "* Calls the `_grow_tree()` function and assigns the output to the `root` attribute. We will see the function `_grow_tree()` below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will look at the `_grow_tree()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _grow_tree(self, X, y, depth=0):\n",
    "    n_samples, n_features = X.shape\n",
    "    n_labels = len(np.unique(y))\n",
    "\n",
    "    # stopping criteria\n",
    "    if (\n",
    "        depth >= self.max_depth\n",
    "        or n_labels == 1\n",
    "        or n_samples < self.min_samples_split\n",
    "    ):\n",
    "        leaf_value = self._most_common_label(y)\n",
    "        return Node(value=leaf_value)\n",
    "\n",
    "    feat_idxs = np.random.choice(n_features, self.n_feats, replace=False)\n",
    "\n",
    "    # greedily select the best split according to information gain\n",
    "    best_feat, best_thresh = self._best_criteria(X, y, feat_idxs)\n",
    "\n",
    "    # grow the children that result from the split\n",
    "    left_idxs, right_idxs = self._split(X[:, best_feat], best_thresh)\n",
    "    left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth + 1)\n",
    "    right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth + 1)\n",
    "    return Node(best_feat, best_thresh, left, right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `_grow_tree()` function takes in 3 inputs:\n",
    "\n",
    "* `X`: Represents the input features of the training data within the node to be split.\n",
    "\n",
    "* `y`: Denotes the labels of the training data within the node to be split.\n",
    "\n",
    "* `depth`: Indicates the depth within the tree. It ensures that the tree is fitted only up to the specified `max_depth`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `_grow_tree()` function performs the following tasks:\n",
    "\n",
    "* Line 2 computes the dimensions of the input data.\n",
    "* Line 3 calculates the number of unique labels present in the input data.\n",
    "* Line 6 checks the stopping criteria:\n",
    "  - Line 7: If the depth of the tree has reached `max_depth`.\n",
    "  - Line 8: If there is only one unique label present in the node (i.e., entropy equals 0).\n",
    "  - Line 9: If the number of data points in the node is less than `min_samples_split`.\n",
    "* Line 11: If any of the conditions in the if statement on line 6 evaluates to True:\n",
    "  - We have reached a leaf node.\n",
    "  - The most common label in the node is computed and assigned as the leaf label.\n",
    "* Line 14: A subset of features is randomly chosen for splitting. This is particularly useful for random forest where feature selection is randomized at each node.\n",
    "* Line 17: The best split is determined by calling the `_best_criteria` function.\n",
    "* Line 20: The data point indexes that belong to the left and right nodes after the split are computed.\n",
    "* Lines 21 and 22: The `_grow_tree()` function is recursively called on the data in the left and right nodes.\n",
    "* Line 23: The subtrees built in the left and right nodes are used to construct the overall tree, with the current node as the root. The result is then returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we examine the `_best_criteria()` function, which identifies the best feature and splitting point. It accepts the input data along with the list of feature indexes designated for splitting in this node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _best_criteria(self, X, y, feat_idxs):\n",
    "    best_gain = -1\n",
    "    split_idx, split_thresh = None, None\n",
    "    for feat_idx in feat_idxs:\n",
    "        X_column = X[:, feat_idx]\n",
    "        thresholds = np.unique(X_column) # simple and exhaustive\n",
    "        for threshold in thresholds:\n",
    "            gain = self._information_gain(y, X_column, threshold)\n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                split_idx = feat_idx\n",
    "                split_thresh = threshold\n",
    "\n",
    "    return split_idx, split_thresh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, `_best_criteria()` function executes the following steps:\n",
    "* Line 5: Retrieves the column on which we are evaluating splits and stores it in the variable `X_column`. \n",
    "* Line 6: Computes all the unique values present in `X_column`, which will serve as our splitting thresholds.\n",
    "* Line 7: In a loop over different thresholds:\n",
    "  - Line 8: Calculates the gain as the information gain by calling the `_information_gain()` function.\n",
    "  - Line 9: If the current gain is greater than the best gain observed so far, updates the current gain as the best gain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's examine the `_information_gain()` function. This function accepts the data labels `y`, the column we are evaluating for splitting `X_column`, and the threshold under consideration `split_thresh`. Its purpose is to compute the information gain if we were to choose that split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _information_gain(self, y, X_column, split_thresh):\n",
    "    # parent loss\n",
    "    parent_entropy = entropy(y)\n",
    "\n",
    "    # generate split\n",
    "    left_idxs, right_idxs = self._split(X_column, split_thresh)\n",
    "\n",
    "    if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
    "        return 0\n",
    "\n",
    "    # compute the weighted avg. of the loss for the children\n",
    "    n = len(y)\n",
    "    n_l, n_r = len(left_idxs), len(right_idxs)\n",
    "    e_l, e_r = entropy(y[left_idxs]), entropy(y[right_idxs])\n",
    "    child_entropy = (n_l / n) * e_l + (n_r / n) * e_r\n",
    "\n",
    "    # information gain is difference in loss before vs. after split\n",
    "    ig = parent_entropy - child_entropy\n",
    "    return ig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, `_information_gain()` function executes the following steps:\n",
    "* Line 3: Computes the entropy in the parent.\n",
    "* Line 6: Calculates the indexes of data points in the left and right nodes if the split under consideration is implemented.\n",
    "* Line 8: Returns 0 if either child node ends up without any data points after the split.\n",
    "* Lines 12 to 15: Computes the weighted average of child entropies based on the discussed formula.\n",
    "* Line 19: Returns the difference between the parent entropy and the child entropy as the information gain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's direct our attention to the `_split()` function, which determines the distribution of data samples into the right or left child nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split(self, X_column, split_thresh):\n",
    "    left_idxs = np.argwhere(X_column <= split_thresh).flatten()\n",
    "    right_idxs = np.argwhere(X_column > split_thresh).flatten()\n",
    "    return left_idxs, right_idxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `_split()` function takes in the column we are evaluating for splitting, as well as the threshold. It identifies the indexes where the value in `X_column` is less than or equal to or greater than the `split_thresh`, and returns them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the most common label value, we utilize the `_most_common_label()` function. Let's delve into how this function operates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _most_common_label(self, y):\n",
    "    from collections import Counter\n",
    "    counter = Counter(y)\n",
    "    most_common = counter.most_common(1)[0][0]\n",
    "    return most_common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function simply counts the repetitions of all labels in `y`. Chooses the most frequent one and returns it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've completed the functions responsible for fitting the tree. Finally, let's explore how we perform predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, X):\n",
    "    return np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "\n",
    "def _traverse_tree(self, x, node):\n",
    "    if node.is_leaf_node():\n",
    "        return node.value\n",
    "\n",
    "    if x[node.feature] <= node.threshold:\n",
    "        return self._traverse_tree(x, node.left)\n",
    "    return self._traverse_tree(x, node.right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During prediction, we invoke the `predict()` function, which accepts the testing data. As shown in line 2, it iterates through all the data points in the test data `X`, calling the _traverse_tree() function.\n",
    "\n",
    "The `_traverse_tree()` function, defined on line 4, is a recursive function that takes a single test data point and the current node (initially the root). If the current node is a leaf node, it returns the value of the leaf node as the prediction. If not, based on the splitting criteria of that node, it recursively calls the `_traverse_tree()` function on either the left or the right child node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting all we developed together in a single class, we will have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def entropy(y):\n",
    "    hist = np.bincount(y)\n",
    "    ps = hist / len(y)\n",
    "    return -np.sum([p * np.log2(p) for p in ps if p > 0])\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(\n",
    "        self, feature=None, threshold=None, left=None, right=None, *, value=None\n",
    "    ):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "\n",
    "    def is_leaf_node(self):\n",
    "        return self.value is not None\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, min_samples_split=2, max_depth=100, n_feats=None):\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.n_feats = n_feats\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.n_feats = X.shape[1] if not self.n_feats else min(self.n_feats, X.shape[1])\n",
    "        self.root = self._grow_tree(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "        n_labels = len(np.unique(y))\n",
    "\n",
    "        # stopping criteria\n",
    "        if (\n",
    "            depth >= self.max_depth\n",
    "            or n_labels == 1\n",
    "            or n_samples < self.min_samples_split\n",
    "        ):\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return Node(value=leaf_value)\n",
    "\n",
    "        feat_idxs = np.random.choice(n_features, self.n_feats, replace=False)\n",
    "\n",
    "        # greedily select the best split according to information gain\n",
    "        best_feat, best_thresh = self._best_criteria(X, y, feat_idxs)\n",
    "\n",
    "        # grow the children that result from the split\n",
    "        left_idxs, right_idxs = self._split(X[:, best_feat], best_thresh)\n",
    "        left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth + 1)\n",
    "        right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth + 1)\n",
    "        return Node(best_feat, best_thresh, left, right)\n",
    "\n",
    "    def _best_criteria(self, X, y, feat_idxs):\n",
    "        best_gain = -1\n",
    "        split_idx, split_thresh = None, None\n",
    "        for feat_idx in feat_idxs:\n",
    "            X_column = X[:, feat_idx]\n",
    "            thresholds = np.unique(X_column)\n",
    "            for threshold in thresholds:\n",
    "                gain = self._information_gain(y, X_column, threshold)\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_idx = feat_idx\n",
    "                    split_thresh = threshold\n",
    "\n",
    "        return split_idx, split_thresh\n",
    "\n",
    "    def _information_gain(self, y, X_column, split_thresh):\n",
    "        # parent loss\n",
    "        parent_entropy = entropy(y)\n",
    "\n",
    "        # generate split\n",
    "        left_idxs, right_idxs = self._split(X_column, split_thresh)\n",
    "\n",
    "        if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
    "            return 0\n",
    "\n",
    "        # compute the weighted avg. of the loss for the children\n",
    "        n = len(y)\n",
    "        n_l, n_r = len(left_idxs), len(right_idxs)\n",
    "        e_l, e_r = entropy(y[left_idxs]), entropy(y[right_idxs])\n",
    "        child_entropy = (n_l / n) * e_l + (n_r / n) * e_r\n",
    "\n",
    "        # information gain is difference in loss before vs. after split\n",
    "        ig = parent_entropy - child_entropy\n",
    "        return ig\n",
    "\n",
    "    def _split(self, X_column, split_thresh):\n",
    "        left_idxs = np.argwhere(X_column <= split_thresh).flatten()\n",
    "        right_idxs = np.argwhere(X_column > split_thresh).flatten()\n",
    "        return left_idxs, right_idxs\n",
    "\n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.is_leaf_node():\n",
    "            return node.value\n",
    "\n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        return self._traverse_tree(x, node.right)\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        counter = Counter(y)\n",
    "        most_common = counter.most_common(1)[0][0]\n",
    "        return most_common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a Dataset and Testing Our Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first import the breast cancer dataset and `train_test_split` function from the `scikit-learn` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we prepare the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be assessing our model with accuracy. Let's define it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we initiate a `DecisionTree` instance, fit it, use it for prediction, and obtain the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9210526315789473\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTree(max_depth=10)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "acc = accuracy(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, what we built is pretty accurate!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What We Have Learned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this discussion, we did the following:\n",
    "* Revisiting the core principles of learning decision trees.\n",
    "* Importing a dataset from `scikit-learn`.\n",
    "* Generating predictions using the trained tree.\n",
    "* Measuring performance by accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hope you have enjoyed this lesson!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
