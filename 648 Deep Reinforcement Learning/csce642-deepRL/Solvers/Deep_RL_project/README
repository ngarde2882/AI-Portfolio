This code was written by Joseph Wagner and Nicholas Garde with the utilization of several extemeley detailed code bases linked bellow.

written by hsahovic
https://github.com/hsahovic/poke-env.git 
and smogon
https://github.com/smogon/pokemon-showdown.git
and using RL alg's in
https://stable-baselines3.readthedocs.io/en/master/

In order to run the main files in the GIT you must follow the following tutorial 
https://poke-env.readthedocs.io/en/stable/getting_started.html#installing-poke-env
note that this totorial took us a week to set up as you must run your own online server in Node to with the utlization of smogon's code, However if you are familiar with this process these steps will setup and run server via

git clone https://github.com/smogon/pokemon-showdown.git
cd pokemon-showdown
npm install
cp config/config-example.js config/config.js
node pokemon-showdown start --no-security

Then to run our Baseline.py which we will compair our code simply run, 
Baseline.py

To test each indivuial algorthium that we wrote run 
Joe's RL.py
Nick's RL.py
These files will give you the working renforcement learning algorihtiums for each different training regiment that we developed.

The key variables to change in Modified Rl Alg.1.py file are
"NB_RANDOM_TRAINING_STEPS=50_000" on line 352
"model = DQN("MlpPolicy", env_player, verbose=1)" 364
"opponent = MaxDamagePlayer()" on line 547
Changing NB_RANDOM_TRAINING_STEPS changes the number of traing sets that the RL alg undergoes
Changing model = DQN("MlpPolicy", env_player, verbose=1) changes the RL algorthium used with A2C and DQN being available
and changing opponent changes what opponent we play, which can be changed between RandomPlayer(),MaxDamagePlayer(), and SimpleHeuristicsPlayer()

The key variables to change in Modified Rl Alg.2.py file are withing the "__main__" on on line 562
"a2c_training(2_000_000)"
"dqn_training(500000)"

"a2c_evaluation(csv=False)"
"dqn_evaluation(csv=False)"

Changing a2c_training(2_000_000) changes the number of traing sets that the RL alg undergoes for the A2C algorthium and saves the model for the number of training steps
Changing dqn_training(1_000_000) changes the number of traing sets that the RL alg undergoes for the DQN algorthium and saves the model for the number of training steps

where a2c_evaluation(csv=False) computes the results for different opponents
where dqn_evaluation(csv=False) computes the results for different opponents

Note that for most cases we can only evaulate one opponent at a time. 

To run our code online and play each RL player you need to run the Online_Project_RL.py file which will allow you to challange our model online and play against it. 
If you would like to play our model please email me at Joewag@tamu.edu and I will allow you to chanlange our model on 
https://play.pokemonshowdown.com/

Watch the Battles we did play in Battle replays,simply paste the link into your web browser



