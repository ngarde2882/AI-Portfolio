{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DF (10980480, 5)\n",
      "Index(['n', 'k', 'm', 'result', 'P'], dtype='object')\n",
      "    n  k  m       result                                                  P\n",
      "0  10  5  2   266.264524  [-53.30165452517249, 46.36747377721454, 12.267...\n",
      "1   9  4  2   140.163560  [6.5178894459618135, -6.281201889271188, 99.66...\n",
      "2  10  4  3   167.692765  [-67.02930762978451, 82.95217006264068, -6.583...\n",
      "3   9  4  3   565.010999  [91.14110355866848, -15.260304068831033, -47.3...\n",
      "4   9  6  2  1170.076493  [92.83943954357875, -97.82331459868368, 5.3277...\n"
     ]
    }
   ],
   "source": [
    "from joblib import load\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# fetch training data (thank you keegan)\n",
    "df = load('../../DL_data/results_dataframe/results_dataframe.pkl')\n",
    "print(type(df))\n",
    "print(f'DF {df.shape}')\n",
    "print(df.columns)\n",
    "\n",
    "def normalize_P(df):\n",
    "    def norm(x):\n",
    "        x = np.array(x, dtype=np.float32)\n",
    "        return ((x - -100) / (100 - -100)) * 2 - 1\n",
    "    return df['P'].apply(lambda x: norm(x).tolist())\n",
    "# df['P'] = normalize_P(df)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "class RowTokenDataset(Dataset):\n",
    "    def __init__(self, df, n, k):\n",
    "        self.n = n\n",
    "        self.k = k\n",
    "        self.m = df['m'].astype(int).values\n",
    "        self.P = df['P'].apply(lambda s: torch.tensor(s, dtype=torch.float32))\n",
    "        # self.Pt = df['P'].apply(lambda s: torch.tensor(s, dtype=torch.float32).transpose(0, 1))\n",
    "        self.y = torch.tensor(df['result'].values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve the tensor P\n",
    "        P_val = self.P.iloc[idx].view(self.k, self.n - self.k)\n",
    "        target = self.y[idx]\n",
    "        \n",
    "        # Create an m token that has the same dimension as a row in P.\n",
    "        token_dim = P_val.size(1)  # expected token dim = n-k\n",
    "        m_token = torch.full((1, token_dim), self.m[idx], dtype=torch.float32)\n",
    "        \n",
    "        # Combine the tokens: place the m token at the start.\n",
    "        tokens = torch.cat([m_token, P_val], dim=0)  # shape: (k+1, n-k)\n",
    "        return tokens, target\n",
    "\n",
    "def process_P(s, k, n):\n",
    "    t = torch.tensor(s, dtype=torch.float32)\n",
    "    if t.dim() == 1:\n",
    "        t = t.view(k, n - k)\n",
    "    return t.transpose(0, 1)\n",
    "\n",
    "class ColTokenDataset(Dataset):\n",
    "    def __init__(self, df, n, k):\n",
    "        self.n = n\n",
    "        self.k = k\n",
    "        self.m = df['m'].astype(int).values\n",
    "        self.Pt = df['P'].apply(lambda s: process_P(s, k, n))\n",
    "        self.y = torch.tensor(df['result'].values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve the tensor P\n",
    "        Pt_val = self.Pt.iloc[idx].view(self.n - self.k, self.k)\n",
    "        target = self.y[idx]\n",
    "        \n",
    "        # Create an m token that has the same dimension as a row in P.\n",
    "        token_dim = Pt_val.size(1)  # expected token dim = n-k\n",
    "        m_token = torch.full((1, token_dim), self.m[idx], dtype=torch.float32)\n",
    "        \n",
    "        # Combine the tokens: place the m token at the start.\n",
    "        tokens = torch.cat([m_token, Pt_val], dim=0)  # shape: (k+1, n-k)\n",
    "        return tokens, target\n",
    "\n",
    "class RowColTokenDataset(Dataset):\n",
    "    def __init__(self, df, n, k):\n",
    "        self.n = n\n",
    "        self.k = k\n",
    "        self.m = df['m'].astype(int).values\n",
    "        self.P = df['P'].apply(lambda s: torch.tensor(s, dtype=torch.float32))\n",
    "        self.Pt = df['P'].apply(lambda s: process_P(s, k, n))\n",
    "\n",
    "        # Optional target\n",
    "        self.has_targets = 'result' in df.columns\n",
    "        if self.has_targets:\n",
    "            self.y = torch.tensor(df['result'].values, dtype=torch.float32)\n",
    "\n",
    "        if k < (n - k):\n",
    "            self.pad_col = (0, (n - k) - k)\n",
    "            self.pad_row = None\n",
    "        elif k > (n - k):\n",
    "            self.pad_row = (0, k - (n - k))\n",
    "            self.pad_col = None\n",
    "        else:\n",
    "            self.pad_row = None\n",
    "            self.pad_col = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.m)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        P_val = self.P.iloc[idx].view(self.k, self.n - self.k)\n",
    "        Pt_val = self.Pt.iloc[idx].view(self.n - self.k, self.k)\n",
    "\n",
    "        if self.pad_row:\n",
    "            P_val = F.pad(P_val, self.pad_row, mode='constant', value=0)\n",
    "        if self.pad_col:\n",
    "            Pt_val = F.pad(Pt_val, self.pad_col, mode='constant', value=0)\n",
    "\n",
    "        tokens = torch.cat([P_val, Pt_val], dim=0)\n",
    "\n",
    "        if self.has_targets:\n",
    "            return tokens, self.y[idx]\n",
    "        else:\n",
    "            return tokens, torch.tensor(0.0)\n",
    "\n",
    "# Positional encoding module from \"Attention is All You Need\"\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, d_model)\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Transformer model for regression\n",
    "class TransformerRegression(nn.Module):\n",
    "    def __init__(self, token_dim, d_model, nhead, num_layers, dim_feedforward, n, k, m, dropout=0.1):\n",
    "        \"\"\"\n",
    "        token_dim: Dimension of each token (n-k from your dataset)\n",
    "        d_model: Embedding dimension for the transformer\n",
    "        nhead: Number of attention heads\n",
    "        num_layers: Number of transformer encoder layers\n",
    "        dim_feedforward: Dimension of the feed-forward network inside the transformer\n",
    "        dropout: Dropout rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n = n; self.k = k; self.m = m\n",
    "        # Project input tokens (of dimension token_dim) to d_model dimension.\n",
    "        self.input_projection = nn.Linear(token_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # For regression, we can aggregate transformer outputs.\n",
    "        # Here, we use mean pooling across the sequence.\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model//2, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape: (batch_size, seq_len, token_dim)\n",
    "        \"\"\"\n",
    "        # Project the tokens into embedding space.\n",
    "        x = self.input_projection(x)  # (batch_size, seq_len, d_model)\n",
    "        x = self.pos_encoder(x)\n",
    "        \n",
    "        # Transformer expects shape (seq_len, batch_size, d_model)\n",
    "        x = x.transpose(0, 1)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.transpose(0, 1)  # back to (batch_size, seq_len, d_model)\n",
    "        \n",
    "        # Mean pool over the sequence dimension\n",
    "        pooled = x.mean(dim=1)\n",
    "        output = self.regressor(pooled)\n",
    "        return output\n",
    "\n",
    "def ratio_loss(preds, targets, eps=1e-6):\n",
    "    preds = torch.clamp(preds, min=eps)\n",
    "    targets = torch.clamp(targets, min=eps)\n",
    "    return torch.mean((torch.log2(targets) - torch.log2(preds))**2)\n",
    "criterion = ratio_loss\n",
    "\n",
    "LENIENT = True\n",
    "def train(model, train_loader, val_loader, config):\n",
    "    train_losses, val_losses = [], []\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    no_improve_epochs = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        for tokens, target in train_loader:\n",
    "            tokens, target = tokens.to(device), target.to(device).unsqueeze(1)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(tokens)\n",
    "            loss = criterion(preds, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * tokens.size(0)\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for tokens, target in val_loader:\n",
    "                tokens, target = tokens.to(device), target.to(device).unsqueeze(1)\n",
    "                preds = model(tokens)\n",
    "                loss = criterion(preds, target)\n",
    "                running_val_loss += loss.item() * tokens.size(0)\n",
    "        val_loss = running_val_loss / len(val_loader.dataset)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}: train loss {train_loss:.4f}, val loss {val_loss:.4f}\")\n",
    "\n",
    "        # Save model if validation loss improves\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            torch.save({\n",
    "                'model_state_dict': best_model_state,\n",
    "                'config': config\n",
    "            }, f\"model2{config['flag']}-{model.n}_{model.k}_{model.m}.pt\")\n",
    "            print(f\"Best model saved at epoch {epoch+1} with val loss: {best_val_loss:.4f}\")\n",
    "            no_improve_epochs = 0\n",
    "        else: no_improve_epochs+=1\n",
    "\n",
    "        if LENIENT: # break after 5 epochs of no improvement\n",
    "            if no_improve_epochs > 4: print('terminating training'); break\n",
    "            # early break a model that won't learn\n",
    "            if epoch>2 and val_loss>500: print('terminating training'); break\n",
    "        else: # break after 3 epochs of no improvement\n",
    "            if no_improve_epochs > 2: print('terminating training'); break\n",
    "    # Plot losses\n",
    "    plt.figure()\n",
    "    plt.plot(train_losses, label=\"Train loss\")\n",
    "    plt.plot(val_losses, label=\"Val loss\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.legend()\n",
    "    plt.title(f\"Training and Validation Losses (n={model.n}, k={model.k})\")\n",
    "    plt.show()\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    return model\n",
    "\n",
    "\n",
    "def test_and_plot(model, test_loader):\n",
    "    model.eval()\n",
    "\n",
    "    all_preds, all_targets = [], []\n",
    "    total_loss = 0\n",
    "    criterion = ratio_loss\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for tokens, target in test_loader:\n",
    "            # tokens shape: (batch, seq_len, token_dim)\n",
    "            tokens = tokens.to(device)\n",
    "            target = target.to(device).unsqueeze(1)\n",
    "            preds = model(tokens).reshape(-1)\n",
    "            loss = criterion(preds, target.reshape(-1))\n",
    "            total_loss += loss.item() * tokens.size(0)\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_targets.append(target.cpu().reshape(-1))\n",
    "            # Extract m values from the first token.\n",
    "            # Assume the m token is the first row of each sample and each token is a constant vector.\n",
    "            # We can take the first element as representative.\n",
    "\n",
    "    all_preds = torch.cat(all_preds).numpy()\n",
    "    all_targets = torch.cat(all_targets).numpy()\n",
    "\n",
    "    loss = total_loss / len(test_loader.dataset)\n",
    "    print(f\"Test Ratio Loss: {loss:.6f}\")\n",
    "\n",
    "    # graph bounds\n",
    "    combined = np.concatenate([all_targets, all_preds])\n",
    "    low, high = np.percentile(combined, [0, 99])\n",
    "\n",
    "    plt.figure()\n",
    "    sc = plt.scatter(all_targets, all_preds, alpha=0.7)\n",
    "    plt.plot([low, high], [low, high], linestyle='--', label='Ideal')\n",
    "    plt.xlim(low, high); plt.ylim(low, high)\n",
    "    plt.xlabel(\"True target\"); plt.ylabel(\"Predicted\")\n",
    "    plt.title(f\"Test: True vs Predicted on Model n={model.n}, k={model.k}, m={model.m}\")\n",
    "    plt.show()\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "set_seed(1706)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 40\n",
    "# for n, k, m in [(9,4,5), (9,5,4), (9,6,3), (10,4,2), (10,4,6), (10,5,2), (10,5,5), (10,6,4)]:\n",
    "# for n in [9,10]:\n",
    "#     for k in [4,5,6]:\n",
    "#         for m in list(range(n-k+1))[2:]:\n",
    "\n",
    "# LENIENT = True\n",
    "# for n in [9,10]:\n",
    "#     for k in [4,5,6]:\n",
    "#         for m in list(range(n-k+1))[2:]:\n",
    "#             if device.type == \"cuda\":\n",
    "#                 data = df[(df['n'] == n) & (df['k'] == k) & (df['m'] == m)].head(500000)\n",
    "#             else: # <3 laptop\n",
    "#                 data = df[(df['n'] == n) & (df['k'] == k)].head(20000)\n",
    "#             num_train_samples = int(0.65 * len(data))\n",
    "#             num_val_samples = int(0.25 * len(data))\n",
    "#             num_test_samples = len(data) - num_train_samples - num_val_samples\n",
    "#             print(f'n:{n}, k:{k}, m:{m}')\n",
    "#             print(\"num_train_samples:\", num_train_samples,\"num_val_samples:\", num_val_samples,\"num_test_samples:\", num_test_samples)\n",
    "#             dataset = RowColTokenDataset(data, n, k)\n",
    "#             train_data, val_data, test_data = random_split(dataset, [num_train_samples, num_val_samples, num_test_samples])\n",
    "#             train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "#             val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "#             test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "#             if dataset.__class__.__name__ == 'RowTokenDataset': token_dim = n-k\n",
    "#             elif dataset.__class__.__name__ == 'ColTokenDataset': token_dim = k\n",
    "#             else: token_dim = max(k, n - k)\n",
    "#             config = {\n",
    "#                 'n': n,\n",
    "#                 'k': k,\n",
    "#                 'm': m,\n",
    "#                 'token_dim': token_dim,\n",
    "#                 'd_model': 1024,\n",
    "#                 'nhead': 16,\n",
    "#                 'num_layers': 4,\n",
    "#                 'dim_feedforward': 2048,\n",
    "#                 'dropout': 0.2,\n",
    "#                 'lr': 0.00001,\n",
    "#                 'flag': 'd'\n",
    "#             }\n",
    "#             model_args = {key: config[key] for key in [\n",
    "#                 'token_dim', 'd_model', 'nhead', 'num_layers',\n",
    "#                 'dim_feedforward', 'dropout', 'n', 'k', 'm'\n",
    "#             ]}\n",
    "#             model = TransformerRegression(**model_args).to(device)\n",
    "#             optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])\n",
    "#             model = train(model, train_loader, val_loader, config=config)\n",
    "#             test_and_plot(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A\n",
    "'d_model': 512,\\\n",
    "'nhead': 8,\\\n",
    "'num_layers': 4,\\\n",
    "'dim_feedforward': 256,\\\n",
    "'dropout': 0.2,\\\n",
    "'lr': 0.000001,\\\n",
    "'flag': 'a'\\\n",
    "(9,4) Test Loss by m[2:]\\\n",
    ".2, .25, .9, 12.5\\\n",
    "(9,5) Test Loss by m[2:]\\\n",
    ".23, .87, 14.7\\\n",
    "(9,6) Test Loss by m[2:]\\\n",
    ".66, 8.9\\\n",
    "(10,4) Test Loss by m[2:]\\\n",
    ".85, .11, .29, 1.48, 24.6\\\n",
    "(10,5) Test Loss by m[2:]\\\n",
    ".13, .33, .97, 24.5\\\n",
    "(10,6) Test Loss by m[2:]\\\n",
    ".25, .89, 1297\n",
    "\n",
    "# B\n",
    "'d_model': 1024,\\\n",
    "'nhead': 16,\\\n",
    "'num_layers': 6,\\\n",
    "'dim_feedforward': 512,\\\n",
    "'dropout': 0.2,\\\n",
    "'lr': 0.0001,\\\n",
    "'flag': 'b'\\\n",
    "(9,4) Test Loss by m[2:]\\\n",
    "735.6, .26, 891, 3.4\\\n",
    "(9,5) Test Loss by m[2:]\\\n",
    ".25, 903, 3.48\\\n",
    "(9,6) Test Loss by m[2:]\\\n",
    ".66, 3.4\\\n",
    "(10,4) Test Loss by m[2:]\\\n",
    ".87, .12, .3, .95, 3.4\\\n",
    "(10,5) Test Loss by m[2:]\\\n",
    "770, .35, .95, 3.4\\\n",
    "(10,6) Test Loss by m[2:]\\\n",
    ".27, .9, 1297\n",
    "\n",
    "# C\n",
    "'d_model': 1024,\\\n",
    "'nhead': 32,\\\n",
    "'num_layers': 8,\\\n",
    "'dim_feedforward': 2048,\\\n",
    "'dropout': 0.2,\\\n",
    "'lr': 0.00005,\\\n",
    "'flag': 'c'\\\n",
    "(9,4) Test Loss by m[2:]\\\n",
    ".21, 785, .91, 3.4\\\n",
    "(9,5) Test Loss by m[2:]\\\n",
    ".23, .88, 3.4\\\n",
    "(9,6) Test Loss by m[2:]\\\n",
    ".67, 3.4\\\n",
    "(10,4) Test Loss by m[2:]\\\n",
    ".87, .12, .3, 921, 3.4\\\n",
    "(10,5) Test Loss by m[2:]\\\n",
    ".14, .35, .95, 3.5\\\n",
    "(10,6) Test Loss by m[2:]\\\n",
    ".26, .89, 3.5\n",
    "\n",
    "# D\n",
    "'d_model': 1024,\\\n",
    "'nhead': 16,\\\n",
    "'num_layers': 4,\\\n",
    "'dim_feedforward': 2048,\\\n",
    "'dropout': 0.2,\\\n",
    "'lr': 0.00001,\\\n",
    "'flag': 'd'\\\n",
    "(9,4) Test Loss by m[2:]\\\n",
    ".2, 785, .91, 3.4\\\n",
    "(9,5) Test Loss by m[2:]\\\n",
    ".24, .88, \\\n",
    "Terminated training.\n",
    "\n",
    "The larger models cut down on error at the higher m-values. Every model became a 1-value predictor for the optimal output to minimize overall loss. Rather than learning a pattern, these models (often) found the best single value to guess to minimize validation loss.\n",
    "\n",
    "Final ensemble:\\\n",
    "(9,4)[2,3,4] a\\\n",
    "(9,4)[5] c\\\n",
    "(9,5) c\\\n",
    "(9,6) b\\\n",
    "(10,4) b\\\n",
    "(10,5) c\\\n",
    "(10,7) c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def load_ensemble_models(model_dir, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    ensemble_models = {}\n",
    "    for fname in os.listdir(model_dir):\n",
    "        if fname.startswith(\"model2-\") and fname.endswith(\".pt\"):\n",
    "            try:\n",
    "                parts = fname[len(\"model2-\"):-3].split(\"_\")\n",
    "                n, k, m = map(int, parts)\n",
    "                path = os.path.join(model_dir, fname)\n",
    "                checkpoint = torch.load(path, map_location=device)\n",
    "                config = checkpoint['config']\n",
    "                model = TransformerRegression(\n",
    "                    token_dim=config['token_dim'],\n",
    "                    d_model=config['d_model'],\n",
    "                    nhead=config['nhead'],\n",
    "                    num_layers=config['num_layers'],\n",
    "                    dim_feedforward=config['dim_feedforward'],\n",
    "                    dropout=config['dropout'],\n",
    "                    n=config['n'], k=config['k'], m=config['m']\n",
    "                ).to(device)\n",
    "                model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                model.eval()\n",
    "                ensemble_models[(n, k, m)] = model\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load {fname}: {e}\")\n",
    "    return ensemble_models\n",
    "\n",
    "def run_ensemble_predictions(df_test, ensemble_models, batch_size=64):\n",
    "    from collections import defaultdict\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Store results\n",
    "    prediction_results = []\n",
    "\n",
    "    # Group by n, k, m\n",
    "    for (n, k, m), group in df_test.groupby([\"n\", \"k\", \"m\"]):\n",
    "        model = ensemble_models.get((n, k, m))\n",
    "        if model is None:\n",
    "            print(f\"No model for (n={n}, k={k}, m={m}) — skipping\")\n",
    "            continue\n",
    "\n",
    "        dataset = RowColTokenDataset(group.reset_index(drop=True), n=n, k=k)\n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            for tokens, _ in loader:\n",
    "                tokens = tokens.to(device)\n",
    "                pred = model(tokens).squeeze(1)\n",
    "                preds.extend(pred.cpu().numpy().tolist())\n",
    "\n",
    "        result_df = group.copy()\n",
    "        result_df[\"prediction\"] = preds\n",
    "        prediction_results.append(result_df)\n",
    "\n",
    "    if prediction_results:\n",
    "        return pd.concat(prediction_results, ignore_index=True)\n",
    "    else:\n",
    "        return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_test_dataframe(n, k, m, P_list):\n",
    "    \"\"\"\n",
    "    Converts a list of P matrices into a DataFrame compatible with RowColTokenDataset.\n",
    "\n",
    "    Parameters:\n",
    "        n (int): total number of columns in original matrix\n",
    "        k (int): number of rows in each P matrix\n",
    "        m (int or list[int]): the m value(s), scalar or one per row\n",
    "        P_list (list or np.ndarray): one or more numpy matrices of shape (k, n-k)\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame with columns: 'n', 'k', 'm', 'P'\n",
    "    \"\"\"\n",
    "    if isinstance(P_list, np.ndarray) and P_list.ndim == 2:\n",
    "        P_list = [P_list]  # single matrix case\n",
    "\n",
    "    if not isinstance(P_list, list):\n",
    "        raise ValueError(\"P_list must be a list of numpy arrays\")\n",
    "\n",
    "    for i, P in enumerate(P_list):\n",
    "        if P.shape != (k, n - k):\n",
    "            raise ValueError(f\"P[{i}] has shape {P.shape}, expected ({k}, {n - k})\")\n",
    "\n",
    "    # Broadcast m if scalar\n",
    "    m_values = [m] * len(P_list) if isinstance(m, int) else m\n",
    "    if len(m_values) != len(P_list):\n",
    "        raise ValueError(\"Length of m must match number of P matrices\")\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"n\": [n] * len(P_list),\n",
    "        \"k\": [k] * len(P_list),\n",
    "        \"m\": m_values,\n",
    "        \"P\": [p.tolist() for p in P_list]\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "def norm(x):\n",
    "    x = np.array(x, dtype=np.float32)\n",
    "    return ((x + 100) / 200) * 2 - 1\n",
    "\n",
    "n, k, m = 9, 4, 3\n",
    "ensemble_models = load_ensemble_models(\"./\")\n",
    "P_raw = df[(df['n'] == n) & (df['k'] == k) & (df['m'] == m)].head(1)['P'].values[0]\n",
    "P_norm = norm(P_raw).reshape(k, n - k)\n",
    "df_test = build_test_dataframe(n=n, k=k, m=m, P_list=[P_norm])\n",
    "ensemble_output_df = run_ensemble_predictions(df_test, ensemble_models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   n  k  m                                                  P  prediction\n",
      "0  9  4  3  [[0.9114111661911011, -0.15260308980941772, -0...  272.356445\n"
     ]
    }
   ],
   "source": [
    "print(ensemble_output_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
